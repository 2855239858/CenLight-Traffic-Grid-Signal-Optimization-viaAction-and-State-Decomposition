{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Creating Custom Environments 创建自定义环境\n",
    "\n",
    "This tutorial walks you through the process of creating custom environments in Flow. Custom environments contain specific methods that define the problem space of a task, such as the state and action spaces of the RL agent and the signal (or reward) that the RL algorithm will optimize over. By specifying a few methods within a custom environment, individuals can use Flow to design traffic control tasks of various types, such as optimal traffic light signal timing and flow regulation via mixed autonomy traffic (see the figures below). Finally, these environments are compatible with OpenAI Gym.\n",
    "\n",
    "本教程将带您完成在Flow中创建自定义环境的过程。自定义环境包含定义任务的问题空间的特定方法，例如RL代理的状态和操作空间，以及RL算法将优化的信号(或奖励)。通过在自定义环境中指定一些方法，个人可以使用流来设计各种类型的交通控制任务，例如最优的交通灯信号定时和混合自主交通的流量调节(见下图)。最后，这些环境与OpenAI健身房是兼容的。\n",
    "\n",
    "The rest of the tutorial is organized as follows: in section 1 walks through the process of creating an environment for mixed autonomy vehicle control where the autonomous vehicles perceive all vehicles in the network, and section two implements the environment in simulation.\n",
    "\n",
    "本教程的其余部分组织如下:第1节介绍了创建混合自主车辆控制环境的过程，其中自主车辆感知网络中的所有车辆，第2节在仿真中实现了该环境。\n",
    "\n",
    "<img src=\"img/sample_envs.png\">\n",
    "\n",
    "\n",
    "## 1. Creating an Environment Class 创建一个环境类\n",
    "\n",
    "In this tutorial we will create an environment in which the accelerations of a handful of vehicles in the network are specified by a single centralized agent, with the objective of the agent being to improve the average speed of all vehicle in the network. In order to create this environment, we begin by inheriting the base environment class located in *flow.envs*:\n",
    "在本教程中，我们将创建一个环境，其中网络中少数车辆的加速由一个集中的代理指定，代理的目标是提高网络中所有车辆的平均速度。为了创建这样的环境，我们从继承位于*flow.envs*中的基本环境类开始:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the base environment class\n",
    "from flow.envs import Env\n",
    "\n",
    "# define the environment class, and inherit properties from the base environment class\n",
    "class myEnv(Env):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Env` provides the interface for running and modifying a SUMO simulation. Using this class, we are able to start sumo, provide a network to specify a configuration and controllers, perform simulation steps, and reset the simulation to an initial configuration.\n",
    "“Env”提供了运行和修改sumo模拟的接口。使用这个类，我们可以启动sumo，提供指定配置和控制器的网络，执行模拟步骤，并将模拟重置为初始配置。\n",
    "\n",
    "By inheriting Flow's base environment, a custom environment for varying control tasks can be created by adding the following functions to the child class: \n",
    "\n",
    "通过继承Flow的基环境，可以通过在子类中添加以下函数来创建用于变化控制任务的自定义环境:\n",
    "\n",
    "* **action_space**动作空间\n",
    "* **observation_space**观测空间\n",
    "* **apply_rl_actions**RL应用空间\n",
    "* **get_state**获取状态\n",
    "* **compute_reward**计算奖励值\n",
    "\n",
    "Each of these components are covered in the next few subsections.\n",
    "\n",
    "### 1.1 ADDITIONAL_ENV_PARAMS\n",
    "\n",
    "The features used to parametrize components of the state/action space as well as the reward function are specified within the `EnvParams` input, as discussed in tutorial 1. Specifically, for the sake of our environment, the `additional_params` attribute within `EnvParams` will be responsible for storing information on the maximum possible accelerations and decelerations by the autonomous vehicles in the network. Accordingly, for this problem, we define an `ADDITIONAL_ENV_PARAMS` variable of the form:\n",
    "用于参数化状态/动作空间组件的特性以及奖励功能在“EnvParams”输入中指定，如教程1中所述。具体来说，为了保护我们的环境，‘EnvParams’中的‘additional_params’属性将负责存储网络中自动驾驶车辆最大可能的加速和减速信息。因此，对于这个问题，我们定义了表单的‘ADDITIONAL_ENV_PARAMS’变量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_ENV_PARAMS = {\n",
    "    \"max_accel\": 1,\n",
    "    \"max_decel\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All environments presented in Flow provide a unique `ADDITIONAL_ENV_PARAMS` component containing the information needed to properly define some environment-specific parameters. We assume that these values are always provided by the user, and accordingly can be called from `env_params`. For example, if we would like to call the \"max_accel\" parameter, we simply type:\n",
    "Flow中提供的所有环境都提供了一个惟一的‘ADDITIONAL_ENV_PARAMS’组件，其中包含正确定义某些特定于环境的参数所需的信息。我们假设这些值总是由用户提供的，因此可以从' env_params '中调用。例如，如果我们想调用“max_accel”参数，我们只需输入:\n",
    "\n",
    "    max_accel = env_params.additional_params[\"max_accel\"]\n",
    "\n",
    "### 1.2 action_space 动作空间\n",
    "\n",
    "The `action_space` method defines the number and bounds of the actions provided by the RL agent. In order to define these bounds with an OpenAI gym setting, we use several objects located within *gym.spaces*. For instance, the `Box` object is used to define a bounded array of values in $\\mathbb{R}^n$.\n",
    "“action_space”方法定义了RL代理提供的操作的数量和界限。为了定义OpenAI健身房设置的这些边界，我们使用了位于*gym.spaces*内的几个对象。例如，“Box”对象用于定义$\\mathbb{R}^n$中的有界值数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces.box import Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, `Tuple` objects (not used by this tutorial) allow users to combine multiple `Box` elements together.\n",
    "此外，“Tuple”对象(本教程中没有使用)允许用户将多个“Box”元素组合在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have imported the above objects, we are ready to define the bounds of our action space. Given that our actions consist of a list of n real numbers (where n is the number of autonomous vehicles) bounded from above and below by \"max_accel\" and \"max_decel\" respectively (see section 1.1), we can define our action space as follows:\n",
    "\n",
    "一旦导入了上述对象，就可以定义操作空间的边界了。假设我们的动作是由n个实数组成的列表(其中n是自动驾驶车辆的数量)，从上到下分别由“max_accel”和“max_decel”约束(参见1.1节)，我们可以这样定义我们的动作空间:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        num_actions = self.initial_vehicles.num_rl_vehicles\n",
    "        accel_ub = self.env_params.additional_params[\"max_accel\"]\n",
    "        accel_lb = - abs(self.env_params.additional_params[\"max_decel\"])\n",
    "\n",
    "        return Box(low=accel_lb,\n",
    "                   high=accel_ub,\n",
    "                   shape=(num_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 observation_space 观察空间\n",
    "The observation space of an environment represents the number and types of observations that are provided to the reinforcement learning agent. For this example, we will be observe two values for each vehicle: its position and speed. Accordingly, we need a observation space that is twice the size of the number of vehicles in the network.\n",
    "环境的观察空间表示提供给强化学习代理的观察的数量和类型。对于本例，我们将观察每个车辆的两个值:位置和速度。因此，我们需要的观测空间是网络中车辆数量的两倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low=0,\n",
    "            high=float(\"inf\"),\n",
    "            shape=(2*self.initial_vehicles.num_vehicles,),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 apply_rl_actions 应用Rl动作\n",
    "The function `apply_rl_actions` is responsible for transforming commands specified by the RL agent into actual actions performed within the simulator. The vehicle kernel within the environment class contains several helper methods that may be of used to facilitate this process. These functions include:\n",
    "\n",
    "函数' apply_rl_actions '负责将RL代理指定的命令转换为在模拟器中执行的实际操作。environment类中的vehicle内核包含几个辅助方法，可以用来促进这个过程。这些功能包括:\n",
    "\n",
    "* **apply_acceleration** (list of str, list of float) -> None: converts an action, or a list of actions, into accelerations to the specified vehicles (in simulation)\n",
    "* **apply_lane_change** (list of str, list of {-1, 0, 1}) -> None: converts an action, or a list of actions, into lane change directions for the specified vehicles (in simulation)\n",
    "* **choose_route** (list of str, list of list of str) -> None: converts an action, or a list of actions, into rerouting commands for the specified vehicles (in simulation)\n",
    "\n",
    "For our example we consider a situation where the RL agent can only specify accelerations for the RL vehicles; accordingly, the actuation method for the RL agent is defined as follows:\n",
    "\n",
    "在我们的例子中，我们考虑这样一种情况:RL代理只能为RL车辆指定加速;因此，RL agent的驱动方法定义如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        # the names of all autonomous (RL) vehicles in the network\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "\n",
    "        # use the base environment method to convert actions into accelerations for the rl vehicles\n",
    "        self.k.vehicle.apply_acceleration(rl_ids, rl_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 get_state 获取状态\n",
    "\n",
    "The `get_state` method extracts features from within the environments and provides then as inputs to the policy provided by the RL agent. Several helper methods exist within flow to help facilitate this process. Some useful helper method can be accessed from the following objects:\n",
    "\n",
    "“get_state”方法从环境中提取特性，然后作为RL代理提供的策略的输入。flow中存在几个帮助方法来帮助简化这个过程。一些有用的帮助方法可以从以下对象访问:\n",
    "\n",
    "* **self.k.vehicle**: provides current state information for all vehicles within the network为网络中的所有车辆提供当前状态信息\n",
    "* **self.k.traffic_light**: provides state information on the traffic lights提供交通信号灯的状态信息\n",
    "* **self.k.network**: information on the network, which unlike the vehicles and traffic lights is static网络上的信息，这与车辆和红绿灯是静态的\n",
    "* More accessor objects and methods can be found within the Flow documentation at: http://berkeleyflow.readthedocs.io/en/latest/\n",
    "\n",
    "In order to model global observability within the network, our state space consists of the speeds and positions of all vehicles (as mentioned in section 1.3). This is implemented as follows:\n",
    "为了在网络中建立全局可观测性模型，我们的状态空间由所有车辆的速度和位置组成(如第1.3节所述)。实施办法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we use the get_absolute_position method to get the positions of all vehicles\n",
    "        pos = [self.k.vehicle.get_x_by_id(veh_id) for veh_id in ids]\n",
    "\n",
    "        # we use the get_speed method to get the velocities of all vehicles\n",
    "        vel = [self.k.vehicle.get_speed(veh_id) for veh_id in ids]\n",
    "\n",
    "        # the speeds and positions are concatenated to produce the state\n",
    "        return np.concatenate((pos, vel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 compute_reward 计算奖励值\n",
    "\n",
    "The `compute_reward` method returns the reward associated with any given state. These value may encompass returns from values within the state space (defined in section 1.5) or may contain information provided by the environment but not immediately available within the state, as is the case in partially observable tasks (or POMDPs).\n",
    "\n",
    "' compute_reward '方法返回与任何给定状态相关联的奖励。这些值可能包含状态空间(在第1.5节中定义)中的值的返回，或者可能包含环境提供的信息，但是不能立即在状态中使用，就像部分可观察任务(或POMDPs)中的情况一样。\n",
    "\n",
    "For this tutorial, we choose the reward function to be the average speed of all vehicles currently in the network. In order to extract this information from the environment, we use the `get_speed` method within the Vehicle kernel class to collect the current speed of all vehicles in the network, and return the average of these speeds as the reward. This is done as follows:\n",
    "在本教程中，我们选择奖励函数作为当前网络中所有车辆的平均速度。为了从环境中提取这些信息，我们在车辆内核类中使用' get_speed '方法来收集网络中所有车辆的当前速度，并返回这些速度的平均值作为奖励。具体做法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myEnv(myEnv):  # update my environment class\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        # the get_ids() method is used to get the names of all vehicles in the network\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "\n",
    "        # we next get a list of the speeds of all vehicles in the network\n",
    "        speeds = self.k.vehicle.get_speed(ids)\n",
    "\n",
    "        # finally, we return the average of all these speeds as the reward\n",
    "        return np.mean(speeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing the New Environment 测试新环境\n",
    "\n",
    "\n",
    "### 2.1 Testing in Simulation\n",
    "Now that we have successfully created our new environment, we are ready to test this environment in simulation. We begin by running this environment in a non-RL based simulation. The return provided at the end of the simulation is indicative of the cumulative expected reward when jam-like behavior exists within the netowrk. \n",
    "\n",
    "现在我们已经成功地创建了新的环境，我们准备在模拟中测试这个环境。我们首先在一个非基于rl的模拟中运行这个环境。在模拟结束时提供的回报指示了在netowrk中存在类似于jam的行为时累积的预期回报。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "from flow.core.experiment import Experiment\n",
    "from flow.core.params import SumoParams, EnvParams, \\\n",
    "    InitialConfig, NetParams\n",
    "from flow.core.params import VehicleParams\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=True)\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(veh_id=\"idm\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=22)\n",
    "\n",
    "env_params = EnvParams(additional_params=ADDITIONAL_ENV_PARAMS)\n",
    "\n",
    "additional_net_params = ADDITIONAL_NET_PARAMS.copy()\n",
    "net_params = NetParams(additional_params=additional_net_params)\n",
    "\n",
    "initial_config = InitialConfig(bunching=20)\n",
    "\n",
    "flow_params = dict(\n",
    "    exp_tag='ring',\n",
    "    env_name=myEnv,  # using my new environment for the simulation\n",
    "    network=RingNetwork,\n",
    "    simulator='traci',\n",
    "    sim=sim_params,\n",
    "    env=env_params,\n",
    "    net=net_params,\n",
    "    veh=vehicles,\n",
    "    initial=initial_config,\n",
    ")\n",
    "\n",
    "# number of time steps\n",
    "flow_params['env'].horizon = 1500\n",
    "exp = Experiment(flow_params)\n",
    "\n",
    "# run the sumo simulation\n",
    "_ = exp.run(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training the New Environment 培训新环境\n",
    "\n",
    "Next, we wish to train this environment in the presence of the autonomous vehicle agent to reduce the formation of waves in the network, thereby pushing the performance of vehicles in the network past the above expected return.\n",
    "\n",
    "接下来，我们希望在自主车辆代理存在的情况下训练这种环境，以减少网络中波浪的形成，从而使网络中车辆的性能超过上述预期收益。\n",
    "\n",
    "The below code block may be used to train the above environment using the Proximal Policy Optimization (PPO) algorithm provided by RLlib. In order to register the environment with OpenAI gym, the environment must first be placed in a separate \".py\" file and then imported via the script below. Then, the script immediately below should function regularly.\n",
    "\n",
    "下面的代码块可以使用RLlib提供的Proximal Policy Optimization (PPO)算法来训练上述环境。为了注册OpenAI健身房的环境，环境必须首先放在一个单独的。py”。然后通过下面的脚本导入。然后，下面的脚本应该正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "####### Replace this with the environment you created #######\n",
    "#############################################################\n",
    "from flow.envs import AccelEnv as myEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We do not recommend training this environment to completion within a jupyter notebook setting; however, once training is complete, visualization of the resulting policy should show that the autonomous vehicle learns to dissipate the formation and propagation of waves in the network.\n",
    "\n",
    "**注**:我们不建议在这种环境下进行的培训是在木星笔记本设置中完成的;然而，一旦训练完成，结果策略的可视化应该表明，自主车辆学会了在网络中消散波的形成和传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ray\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import SumoParams, EnvParams, InitialConfig, NetParams\n",
    "from flow.core.params import VehicleParams, SumoCarFollowingParams\n",
    "from flow.controllers import RLController, IDMController, ContinuousRouter\n",
    "\n",
    "\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1500\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "\n",
    "\n",
    "# We place one autonomous vehicle and 22 human-driven vehicles in the network\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(\n",
    "    veh_id=\"human\",\n",
    "    acceleration_controller=(IDMController, {\n",
    "        \"noise\": 0.2\n",
    "    }),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        min_gap=0\n",
    "    ),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    num_vehicles=21)\n",
    "vehicles.add(\n",
    "    veh_id=\"rl\",\n",
    "    acceleration_controller=(RLController, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    num_vehicles=1)\n",
    "\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=\"stabilizing_the_ring\",\n",
    "\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=myEnv,  # <------ here we replace the environment with our new environment\n",
    "\n",
    "    # name of the network class the experiment is running on\n",
    "    network=RingNetwork,\n",
    "\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=SumoParams(\n",
    "        sim_step=0.1,\n",
    "        render=True,\n",
    "    ),\n",
    "\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        horizon=HORIZON,\n",
    "        warmup_steps=750,\n",
    "        clip_actions=False,\n",
    "        additional_params={\n",
    "            \"target_velocity\": 20,\n",
    "            \"sort_vehicles\": False,\n",
    "            \"max_accel\": 1,\n",
    "            \"max_decel\": 1,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # network-related parameters (see flow.core.params.NetParams and the\n",
    "    # network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=NetParams(\n",
    "        additional_params=ADDITIONAL_NET_PARAMS.copy()\n",
    "    ),\n",
    "\n",
    "    # vehicles to be placed in the network at the start of a rollout (see\n",
    "    # flow.core.params.VehicleParams)\n",
    "    veh=vehicles,\n",
    "\n",
    "    # parameters specifying the positioning of vehicles upon initialization/\n",
    "    # reset (see flow.core.params.InitialConfig)\n",
    "    initial=InitialConfig(\n",
    "        bunching=20,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def setup_exps():\n",
    "    \"\"\"Return the relevant components of an RLlib experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        name of the training algorithm\n",
    "    str\n",
    "        name of the gym environment to be trained\n",
    "    dict\n",
    "        training configuration parameters\n",
    "    \"\"\"\n",
    "    alg_run = \"PPO\"\n",
    "\n",
    "    agent_cls = get_agent_class(alg_run)\n",
    "    config = agent_cls._default_config.copy()\n",
    "    config[\"num_workers\"] = N_CPUS\n",
    "    config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS\n",
    "    config[\"gamma\"] = 0.999  # discount rate\n",
    "    config[\"model\"].update({\"fcnet_hiddens\": [3, 3]})\n",
    "    config[\"use_gae\"] = True\n",
    "    config[\"lambda\"] = 0.97\n",
    "    config[\"kl_target\"] = 0.02\n",
    "    config[\"num_sgd_iter\"] = 10\n",
    "    config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "    config[\"horizon\"] = HORIZON\n",
    "\n",
    "    # save the flow params for replay\n",
    "    flow_json = json.dumps(\n",
    "        flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "    config['env_config']['flow_params'] = flow_json\n",
    "    config['env_config']['run'] = alg_run\n",
    "\n",
    "    create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "    # Register as rllib env\n",
    "    register_env(gym_name, create_env)\n",
    "    return alg_run, gym_name, config\n",
    "\n",
    "\n",
    "alg_run, gym_name, config = setup_exps()\n",
    "ray.init(num_cpus=N_CPUS + 1)\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 20,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 200,\n",
    "        },\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
