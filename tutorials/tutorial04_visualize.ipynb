{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Visualizing Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial describes the process of visualizing the results of Flow experiments, and of replaying them. \n",
    "\n",
    "本教程描述了可视化流实验结果的过程，以及重放它们的过程。\n",
    "\n",
    "**Note:** This tutorial is only relevant if you use SUMO as a simulator. We currently do not support policy replay nor data collection when using Aimsun. The only exception is for reward plotting, which is independent on whether you have used SUMO or Aimsun during training.\n",
    "\n",
    "**注意:**本教程只适用于您使用sumo作为模拟器的情况。我们目前不支持使用Aimsun时的策略重放和数据收集。唯一的例外是奖励计划，它独立于你在训练中是否使用过相扑或漫无目的的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Visualization components 可视化组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The visualization of simulation results breaks down into three main components:仿真结果可视化主要分为三个部分:\n",
    "\n",
    "- **reward plotting**: Visualization of the reward function is an essential step in evaluating the effectiveness and training progress of RL agents.\n",
    "** *奖励绘制**:奖励函数可视化是评价RL agent有效性和训练进度的重要步骤。\n",
    "\n",
    "- **policy replay**: Flow includes tools for visualizing trained policies using SUMO's GUI. This enables more granular analysis of policies beyond their accrued reward, which in turn allows users to tweak actions, observations and rewards in order to produce some desired behavior. The visualizers also generate plots of observations and a plot of the reward function over the course of the rollout.\n",
    "\n",
    "- **策略重放**:流包括使用SUMO的GUI可视化训练过的策略的工具。这使得可以对策略进行更细粒度的分析，而不仅仅是对其累积的奖励，这反过来又允许用户调整操作、观察和奖励，以产生一些期望的行为。视觉化者还会生成观察图和奖励函数图。\n",
    "\n",
    "- **data collection and analysis**: Any Flow experiment can output its simulation data to a CSV file, `emission.csv`, containing the contents of SUMO's built-in `emission.xml` files. This file contains various data such as the speed, position, time, fuel consumption and many other metrics for every vehicle in the network and at each time step of the simulation. Once you have generated the `emission.csv` file, you can open it and read the data it contains using Python's [csv library](https://docs.python.org/3/library/csv.html) (or using Excel).\n",
    "\n",
    "- **数据收集与分析**:任何流量实验都可以将其模拟数据输出到CSV文件“emission.csv '，包含sumo内置的'发射'内容xml的文件。该文件包含各种数据，如速度、位置、时间、燃料消耗和网络中每辆车的许多其他指标，以及模拟的每个时间步长。一旦你产生了“排放”。您可以使用Python的[csv库](https://docs.python.org/3/library/csv.html)(或使用Excel)打开它并读取它包含的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Visualization is different depending on which reinforcement learning library you are using, if any. Accordingly, the rest of this tutorial explains how to plot rewards, replay policies and collect data when using either no RL library, RLlib, or stable-baselines. \n",
    "\n",
    "可视化是不同的，这取决于你使用的强化学习库，如果有的话。因此，本教程的其余部分将解释如何在不使用RL库、RLlib或稳定基线的情况下绘制奖励、重放策略和收集数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Contents:**\n",
    "\n",
    "[How to visualize using SUMO without training](#2.1---Using-SUMO-without-training)\n",
    "\n",
    "[How to visualize using SUMO with RLlib](#2.2---Using-SUMO-with-RLlib)\n",
    "\n",
    "[**_Example: visualize data on a ring trained using RLlib_**](#2.3---Example:-Visualize-data-on-a-ring-trained-using-RLlib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. How to visualize 如何可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1 - Using SUMO without training \n",
    "\n",
    "_In this case, since there is no training, there is no reward to plot and no policy to replay._\n",
    "在这种情况下，因为没有训练，所以没有奖励去策划，也没有政策去重播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data collection and analysis 数据收集与分析\n",
    "\n",
    "SUMO-only experiments can generate emission CSV files seamlessly:\n",
    "\n",
    "First, you have to tell SUMO to generate the `emission.xml` files. You can do that by specifying `emission_path` in the simulation parameters (class `SumoParams`), which is the path where the emission files will be generated. For instance:\n",
    "\n",
    "SUMO-only实验可以无缝生成排放CSV文件:\n",
    "\n",
    "\n",
    "首先，你必须告诉相扑产生“emission.xml\"的文件。您可以通过在模拟参数(“SumoParams”类)中指定“emission_path”来实现这一点，该参数是生成排放文件的路径。例如:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=True, emission_path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then, you have to tell Flow to convert these XML emission files into CSV files. To do that, pass in `convert_to_csv=True` to the `run` method of your experiment object. For instance:\n",
    "\n",
    "然后，您必须告诉Flow将这些XML发射文件转换为CSV文件。为此，将' convert_to_csv=True '传递给实验对象的' run '方法。例如:\n",
    "\n",
    "```python\n",
    "exp.run(1, convert_to_csv=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When running experiments, Flow will now automatically create CSV files next to the SUMO-generated XML files.\n",
    "\n",
    "在运行实验时，Flow现在将自动在sumo生成的XML文件旁边创建CSV文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 - Using SUMO with RLlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Reward plotting\n",
    "\n",
    "RLlib supports reward visualization over the period of the training using the `tensorboard` command. It takes one command-line parameter, `--logdir`, which is an RLlib result directory. By default, it would be located within an experiment directory inside your `~/ray_results` directory. \n",
    "\n",
    "RLlib支持在训练期间使用“tensorboard”命令进行奖励可视化。它接受一个命令行参数'——logdir '，这是一个RLlib结果目录。默认情况下，它将位于' ~/ray_results '目录下的实验目录中。\n",
    "\n",
    "An example call would look like:一个示例调用如下:\n",
    "\n",
    "`tensorboard --logdir ~/ray_results/experiment_dir/result/directory`\n",
    "\n",
    "You can also run `tensorboard --logdir ~/ray_results` if you want to select more than just one experiment.\n",
    "\n",
    "如果您想要选择多个实验，还可以运行“tensorboard—logdir ~/ray_results”。\n",
    "\n",
    "If you do not wish to use `tensorboard`, an other way is to use our `flow/visualize/plot_ray_results.py` tool. It takes as arguments:\n",
    "\n",
    "如果你不想使用“tensorboard”，另一种方法是使用我们的“flow/ visual/plot_ray_results.py\"工具。它作为参数:\n",
    "\n",
    "- the path to the `progress.csv` file located inside your experiment results directory (`~/ray_results/...`),\n",
    "- the name(s) of the column(s) you wish to plot (reward or other things).\n",
    "\n",
    "通往“progress.csv'文件位于您的实验结果目录中(' ~/ray_results/…')，\n",
    "\n",
    "-您希望绘制的列的名称(奖励或其他东西)。\n",
    "\n",
    "An example call would look like:一个示例调用如下:\n",
    "\n",
    "`flow/visualize/plot_ray_results.py ~/ray_results/experiment_dir/result/progress.csv training/return-average training/return-min`\n",
    "\n",
    "If you do not know what the names of the columns are, run the command without specifying any column:\n",
    "如果您不知道列的名称，请运行该命令，而不指定任何列:\n",
    "\n",
    "`flow/visualize/plot_ray_results.py ~/ray_results/experiment_dir/result/progress.csv`\n",
    "\n",
    "and the list of all available columns will be displayed to you.\n",
    "所有可用列的列表将显示给您。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Policy replay\n",
    "\n",
    "The tool to replay a policy trained using RLlib is located at `flow/visualize/visualizer_rllib.py`. It takes as argument, first the path to the experiment results (by default located within `~/ray_results`), and secondly the number of the checkpoint you wish to visualize (which correspond to the folder `checkpoint_<number>` inside the experiment results directory).\n",
    "\n",
    "使用RLlib训练的策略回放工具位于“flow/ visualizer_rllib.py”处。它作为参数，首先是实验结果的路径(默认位于' ~/ray_results '中)，其次是您希望可视化的检查点的数量(对应于实验结果目录中的文件夹' checkpoint_ ')。\n",
    "\n",
    "An example call would look like this:一个示例调用是这样的:\n",
    "\n",
    "`python flow/visualize/visualizer_rllib.py ~/ray_results/experiment_dir/result/directory 1`\n",
    "\n",
    "There are other optional parameters which you can learn about by running `visualizer_rllib.py --help`. 还可以通过运行“visualizer_rllib.py——help”了解其他可选参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data collection and analysis\n",
    "\n",
    "Simulation data can be generated the same way as it is done [without training](#2.1---Using-SUMO-without-training).\n",
    "模拟数据的生成方法与不经过训练的生成方法相同(#2.1—使用—sumo—不经过训练)。\n",
    "\n",
    "If you need to generate simulation data after the training, you can run a policy replay as mentioned above, and add the `--gen-emission` parameter.\n",
    "如果您需要在培训后生成模拟数据，您可以运行上面提到的策略重播，并添加“—gen-emission”参数。\n",
    "\n",
    "An example call would look like:\n",
    "\n",
    "`python flow/visualize/visualizer_rllib.py ~/ray_results/experiment_dir/result/directory 1 --gen_emission`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Example: Visualize data on a ring trained using RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ryc/flow/tutorials\r\n"
     ]
    }
   ],
   "source": [
    "!pwd  # make sure you are in the flow/tutorials folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder `flow/tutorials/data/trained_ring` contains the data generated in `ray_results` after training an agent on a ring scenario for 200 iterations using RLlib (the experiment can be found in `flow/examples/rllib/stabilizing_the_ring.py`).\n",
    "\n",
    "“flow/tutorials/data/trained_ring”文件夹包含使用RLlib对一个代理进行200次迭代后在“ray_results”中生成的数据(实验可以在“flow/examples/ RLlib / izing_the_ering .py”中找到)。\n",
    "\n",
    "Let's first have a look at what's available in the `progress.csv` file:\n",
    "让我们先来看看“progress.csv”中有哪些内容:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns are: episode_reward_max, episode_reward_min, episode_reward_mean, episode_len_mean, episodes_this_iter, timesteps_this_iter, done, timesteps_total, episodes_total, training_iteration, experiment_id, date, timestamp, time_this_iter_s, time_total_s, pid, hostname, node_ip, time_since_restore, timesteps_since_restore, iterations_since_restore, num_healthy_workers, trial_id, sampler_perf/mean_env_wait_ms, sampler_perf/mean_processing_ms, sampler_perf/mean_inference_ms, info/num_steps_trained, info/num_steps_sampled, info/sample_time_ms, info/load_time_ms, info/grad_time_ms, info/update_time_ms, perf/cpu_util_percent, perf/ram_util_percent, info/learner/default_policy/cur_kl_coeff, info/learner/default_policy/cur_lr, info/learner/default_policy/total_loss, info/learner/default_policy/policy_loss, info/learner/default_policy/vf_loss, info/learner/default_policy/vf_explained_var, info/learner/default_policy/kl, info/learner/default_policy/entropy, info/learner/default_policy/entropy_coeff\r\n"
     ]
    }
   ],
   "source": [
    "!python ../flow/visualize/plot_ray_results.py data/trained_ring/progress.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list of everything that we can plot. Let's plot the reward and its boundaries:\n",
    "这给了我们一个我们可以画的所有东西的列表。让我们来画出奖励和它的边界:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# if this doesn't display anything, try with \"%matplotlib inline\" instead\n",
    "%run ../flow/visualize/plot_ray_results.py data/trained_ring/progress.csv \\\n",
    "episode_reward_mean episode_reward_min episode_reward_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the policy had already converged by the iteration 50.\n",
    "\n",
    "Now let's see what this policy looks like. Run the following script, then click on the green arrow to run the simulation (you may have to click several times).\n",
    "\n",
    "我们可以看到策略在迭代50之前已经收敛了。\n",
    "\n",
    "\n",
    "现在让我们看看这个策略是什么样的。运行以下脚本，然后单击绿色箭头运行模拟(您可能需要多次单击)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../flow/visualize/visualizer_rllib.py data/trained_ring 200 --horizon 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL agent is properly stabilizing the ring! \n",
    "\n",
    "Indeed, without an RL agent, the vehicles start forming stop-and-go waves which significantly slows down the traffic, as you can see in this simulation:\n",
    "\n",
    "RL代理正确地稳定了戒指!\n",
    "\n",
    "\n",
    "事实上，在没有RL代理的情况下，车辆开始形成走走停停的波，这大大减慢了交通，正如你在这个模拟中看到的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../examples/simulate.py ring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trained ring folder, there is a checkpoint generated every 20 iterations. Try to run the second previous command but replace 200 by 20. On the reward plot, you can see that the reward is already quite high at iteration 20, but hasn't converged yet, so the agent will perform a little less well than at iteration 200.\n",
    "\n",
    "在训练的环形文件夹中，每20次迭代生成一个检查点。尝试运行前面的第二个命令，但将200替换为20。在奖励图上，您可以看到在迭代20时奖励已经相当高了，但是还没有收敛，因此代理的表现会比迭代200时稍差一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this example! Feel free to play around with the other scripts in `flow/visualize`. Run them with the `--help` parameter and it should tell you how to use it. Also, if you need the emission file for the trained ring, you can obtain it by running the following command:\n",
    "\n",
    "这就是这个例子!你可以自由地在“流/可视化”中尝试其他脚本。使用“——help”参数运行它们，它应该会告诉您如何使用它。此外，如果你需要训练环的发射文件，你可以通过运行以下命令获得:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ../flow/visualize/visualizer_rllib.py data/trained_ring 200 --horizon 2000 --gen_emission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path where the emission file is generated will be outputted at the end of the simulation.\n",
    "\n",
    "发射文件生成的路径将在仿真结束时输出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
