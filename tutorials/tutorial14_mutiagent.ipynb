{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Multiagent\n",
    "\n",
    "This tutorial covers the implementation of multiagent experiments in Flow. It assumes some level of knowledge or experience in writing custom environments and running experiments with RLlib. The rest of the tutorial is organized as follows. Section 1 describes the procedure through which custom environments can be augmented to generate multiagent environments. Then, section 2 walks you through an example of running a multiagent environment\n",
    "in RLlib.\n",
    "\n",
    "本教程介绍了流中多代理实验的实现。它假定您具有编写自定义环境和使用RLlib运行实验方面的一定知识或经验。本教程的其余部分组织如下。第1节描述了通过扩展自定义环境来生成多代理环境的过程。然后，第2节将介绍一个运行多代理环境的示例\n",
    "\n",
    "在RLlib。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Multiagent Environment Class 创建一个多代理环境类\n",
    "\n",
    "In this part we will be setting up steps to create a multiagent environment. We begin by importing the abstract multi-agent evironment class.\n",
    "\n",
    "在本部分中，我们将设置创建多代理环境的步骤。我们首先导入抽象的多代理环境类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the base Multi-agent environment \n",
    "from flow.envs.multiagent.base import MultiEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiagent experiments, the agent can either share a policy (\"shared policy\") or have different policies (\"non-shared policy\"). In the following subsections, we describe the two.\n",
    "\n",
    "在多代理实验中，代理可以共享一个策略(“共享策略”)，也可以拥有不同的策略(“非共享策略”)。在下面的小节中，我们将介绍这两种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Shared policies 共享策略\n",
    "In the multi-agent environment with a shared policy, different agents will use the same policy. \n",
    "\n",
    "在具有共享策略的多代理环境中，不同的代理将使用相同的策略。\n",
    "\n",
    "We define the environment class, and inherit properties from the Multi-agent version of base env.\n",
    "\n",
    "我们定义了environment类，并从基本环境的多代理版本中继承属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMultiAgentEnv(MultiEnv):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment will provide the interface for running and modifying the multiagent experiment. Using this class, we are able to start the simulation (e.g. in SUMO), provide a network to specify a configuration and controllers, perform simulation steps, and reset the simulation to an initial configuration.\n",
    "\n",
    "该环境将提供运行和修改多代理实验的接口。使用这个类，我们可以启动模拟(例如在SUMO中)，提供一个网络来指定配置和控制器，执行模拟步骤，并将模拟重置为初始配置。\n",
    "\n",
    "For the multi-agent experiments, certain functions of the `MultiEnv` will be changed according to the agents. Some functions will be defined according to a *single* agent, while the other functions will be defined according to *all* agents.\n",
    "\n",
    "在多主体实验中，“MultiEnv”的某些功能会根据主体的不同而改变。一些函数将根据*单个*代理定义，而其他函数将根据*所有*代理定义。\n",
    "\n",
    "In the follwing functions, observation space and action space will be defined for a *single* agent (not all agents):\n",
    "\n",
    "在下面的功能中，将定义一个*单个* agent(不是所有agent)的观察空间和行动空间:\n",
    "\n",
    "* **observation_space**\n",
    "* **action_space**\n",
    "\n",
    "For instance, in a multiagent traffic light grid, if each agents is considered as a single intersection controlling the traffic lights of the intersection, the observation space can be define as *normalized* velocities and distance to a *single* intersection for nearby vehicles, that is defined for every intersection.  \n",
    "\n",
    "例如，在一个多智能体交通信号灯网格中，如果每个智能体都被视为一个控制该交叉口交通灯的单个交叉口，那么对于附近车辆，观测空间可以定义为*归一化*速度和到*单个*交叉口的距离，即为每个交叉口定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_space(self):\n",
    "        \"\"\"State space that is partially observed.\n",
    "\n",
    "        Velocities and distance to intersections for nearby\n",
    "        vehicles ('num_observed') from each direction.\n",
    "        \"\"\"\n",
    "        tl_box = Box(\n",
    "            low=0.,\n",
    "            high=1,\n",
    "            shape=(2 * 4 * self.num_observed),\n",
    "            dtype=np.float32)\n",
    "        return tl_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space can be defined for a *single* intersection as follows\n",
    "可以为一个*单个*交集定义操作空间，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_space(self):\n",
    "        \"\"\"See class definition.\"\"\"\n",
    "        if self.discrete: \n",
    "            # each intersection is an agent, and the action is simply 0 or 1. \n",
    "            # - 0 means no-change in the traffic light \n",
    "            # - 1 means switch the direction\n",
    "            return Discrete(2)\n",
    "        else:\n",
    "            return Box(low=0, high=1, shape=(1,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, the following functions (including their return values) will be defined to take into account *all* agents:\n",
    "\n",
    "相反，下面的函数(包括它们的返回值)将被定义为考虑*所有*代理:\n",
    "\n",
    "* **apply_rl_actions**\n",
    "* **get_state**\n",
    "* **compute_reward**\n",
    "\n",
    "Instead of calculating actions, state, and reward for a single agent, in these functions, the ctions, state, and reward will be calculated for all the agents in the system. To do so, we create a dictionary with agent ids as keys and different parameters (actions, state, and reward ) as vaules. For example, in the following `_apply_rl_actions` function, based on the action of intersections (0 or 1), the state of the intersections' traffic lights will be changed.\n",
    "\n",
    "在这些函数中，将计算系统中所有代理的动作、状态和奖励，而不是计算单个代理的动作、状态和奖励。为此，我们创建了一个字典，其中代理id作为键，不同的参数(动作、状态和奖励)作为变量。例如，在下面的‘_apply_rl_actions’函数中，根据交叉口(0或1)的动作，将改变交叉口交通灯的状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMultiAgentEnv(MultiEnv): \n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        for agent_name in rl_actions:\n",
    "            action = rl_actions[agent_name]\n",
    "            # check if the action space is discrete\n",
    "            \n",
    "            # check if our timer has exceeded the yellow phase, meaning it\n",
    "            # should switch to red\n",
    "            if self.currently_yellow[tl_num] == 1:  # currently yellow\n",
    "                self.last_change[tl_num] += self.sim_step\n",
    "                if self.last_change[tl_num] >= self.min_switch_time: # check if our timer has exceeded the yellow phase, meaning it\n",
    "                # should switch to red\n",
    "                    if self.direction[tl_num] == 0:\n",
    "                        self.k.traffic_light.set_state(\n",
    "                            node_id='center{}'.format(tl_num),\n",
    "                            state=\"GrGr\")\n",
    "                    else:\n",
    "                        self.k.traffic_light.set_state(\n",
    "                            node_id='center{}'.format(tl_num),\n",
    "                            state='rGrG')\n",
    "                    self.currently_yellow[tl_num] = 0\n",
    "            else:\n",
    "                if action:\n",
    "                    if self.direction[tl_num] == 0:\n",
    "                        self.k.traffic_light.set_state(\n",
    "                            node_id='center{}'.format(tl_num),\n",
    "                            state='yryr')\n",
    "                    else:\n",
    "                        self.k.traffic_light.set_state(\n",
    "                            node_id='center{}'.format(tl_num),\n",
    "                            state='ryry')\n",
    "                    self.last_change[tl_num] = 0.0\n",
    "                    self.direction[tl_num] = not self.direction[tl_num]\n",
    "                    self.currently_yellow[tl_num] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `get_state` and `compute_reward` methods support the dictionary structure and add the observation and reward, respectively, as a value for each correpsonding key, that is agent id. \n",
    "\n",
    "类似地，' get_state '和' compute_reward '方法支持字典结构，并分别为每个correpsonding键(即代理id)添加观察值和奖赏值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMultiAgentEnv(MultiEnv): \n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Observations for each intersection\n",
    "\n",
    "        :return: dictionary which contains agent-wise observations as follows:\n",
    "        - For the self.num_observed number of vehicles closest and incomingsp\n",
    "        towards traffic light agent, gives the vehicle velocity and distance to\n",
    "        intersection.\n",
    "        \"\"\"\n",
    "        # Normalization factors\n",
    "        max_speed = max(\n",
    "            self.k.network.speed_limit(edge)\n",
    "            for edge in self.k.network.get_edge_list())\n",
    "        max_dist = max(grid_array[\"short_length\"], grid_array[\"long_length\"],\n",
    "                       grid_array[\"inner_length\"])\n",
    "\n",
    "        # Observed vehicle information\n",
    "        speeds = []\n",
    "        dist_to_intersec = []\n",
    "\n",
    "        for _, edges in self.network.node_mapping:\n",
    "            local_speeds = []\n",
    "            local_dists_to_intersec = []\n",
    "            # .... More code here (removed for simplicity of example)\n",
    "            # ....\n",
    "\n",
    "            speeds.append(local_speeds)\n",
    "            dist_to_intersec.append(local_dists_to_intersec)\n",
    "            \n",
    "        obs = {}\n",
    "        for agent_id in self.k.traffic_light.get_ids():\n",
    "            # .... More code here (removed for simplicity of example)\n",
    "            # ....\n",
    "            observation = np.array(np.concatenate(speeds, dist_to_intersec))\n",
    "            obs.update({agent_id: observation})\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        if rl_actions is None:\n",
    "            return {}\n",
    "\n",
    "        if self.env_params.evaluate:\n",
    "            rew = -rewards.min_delay_unscaled(self)\n",
    "        else:\n",
    "            rew = -rewards.min_delay_unscaled(self) \\\n",
    "                  + rewards.penalize_standstill(self, gain=0.2)\n",
    "\n",
    "        # each agent receives reward normalized by number of lights\n",
    "        rew /= self.num_traffic_lights\n",
    "\n",
    "        rews = {}\n",
    "        for rl_id in rl_actions.keys():\n",
    "            rews[rl_id] = rew\n",
    "        return rews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Non-shared policies 非共享策略\n",
    "\n",
    "In the multi-agent environment with a non-shared policy, different agents will use different policies. In what follows we will see the two agents in a ring road using two different policies, 'adversary' and 'av' (non-adversary).\n",
    "\n",
    "在具有非共享策略的多代理环境中，不同的代理将使用不同的策略。在接下来的内容中，我们将看到这两个代理在一个环形路上使用了两种不同的策略，“对手”和“av”(非对手)。\n",
    "\n",
    "Similarly to the shared policies, observation space and action space will be defined for a *single* agent (not all agents):\n",
    "\n",
    "与共享策略类似，将为一个*单个*代理(不是所有代理)定义观察空间和操作空间:\n",
    "\n",
    "* **observation_space**\n",
    "* **action_space**\n",
    "\n",
    "And, the following functions (including their return values) will be defined to take into account *all* agents::\n",
    "\n",
    "并且，以下函数(包括它们的返回值)将被定义为考虑*所有*代理::\n",
    "\n",
    "* **apply_rl_actions**\n",
    "* **get_state**\n",
    "* **compute_reward**\n",
    "\n",
    "\\* Note that, when observation space and action space will be defined for a single agent, it means that all agents should have the same dimension (i.e. space) of observation and action, even when their policise are not the same. \n",
    "\n",
    "请注意，当为单个agent定义了观察空间和行动空间时，这意味着所有agent都应该具有相同的观察和行动维度(即空间)，即使它们的策略并不相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with defining  `apply_rl_actions` function. In order to make it work for a non-shared policy multi-agent ring road, we define `rl_actions` as a combinations of each policy actions plus the `perturb_weight`.\n",
    "\n",
    "让我们从定义' apply_rl_actions '函数开始。为了使它适用于非共享策略多代理环路，我们将“rl_actions”定义为每个策略操作加上“扰动权值”的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSharedMultiAgentEnv(MultiEnv):\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        # the names of all autonomous (RL) vehicles in the network\n",
    "        agent_ids = [\n",
    "            veh_id for veh_id in self.sorted_ids\n",
    "            if veh_id in self.k.vehicle.get_rl_ids()\n",
    "        ]\n",
    "        # define different actions for different multi-agents \n",
    "        av_action = rl_actions['av']\n",
    "        adv_action = rl_actions['adversary']\n",
    "        perturb_weight = self.env_params.additional_params['perturb_weight']\n",
    "        rl_action = av_action + perturb_weight * adv_action\n",
    "        \n",
    "        # use the base environment method to convert actions into accelerations for the rl vehicles\n",
    "        self.k.vehicle.apply_acceleration(agent_ids, rl_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `get_state` method, we define the state for each of the agents. Remember, the sate of the agents can be different. For the purpose of this example and simplicity, we define the state of the adversary and non-adversary agent to be the same. \n",
    "\n",
    "在“get_state”方法中，我们为每个代理定义状态。记住，代理的状态可以是不同的。为了这个示例和简单起见，我们将对手和非对手代理的状态定义为相同的。\n",
    "\n",
    "In the `compute_reward` method, the agents receive opposing speed rewards. The reward of the adversary agent is more when the speed of the vehicles is small, while the non-adversary agent tries to increase the speeds of the vehicles.\n",
    "\n",
    "在“compute_reward”方法中，代理收到相反的速度奖励。当车辆的速度较小时，敌手的奖励较多，而非敌手则试图提高车辆的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonSharedMultiAgentEnv(MultiEnv):\n",
    "    def get_state(self, **kwargs):\n",
    "        state = np.array([[\n",
    "            self.k.vehicle.get_speed(veh_id) / self.k.network.max_speed(),\n",
    "            self.k.vehicle.get_x_by_id(veh_id) / self.k.network.length()\n",
    "        ] for veh_id in self.sorted_ids])\n",
    "        state = np.ndarray.flatten(state)\n",
    "        return {'av': state, 'adversary': state}\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        if self.env_params.evaluate:\n",
    "            reward = np.mean(self.k.vehicle.get_speed(\n",
    "                self.k.vehicle.get_ids()))\n",
    "            return {'av': reward, 'adversary': -reward}\n",
    "        else:\n",
    "            reward = rewards.desired_velocity(self, fail=kwargs['fail'])\n",
    "            return {'av': reward, 'adversary': -reward}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running Multiagent Environment in RLlib 在RLlib中运行多代理环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the experiment that uses a multiagent environment, we specify certain parameters in the `flow_params` dictionary. \n",
    "\n",
    "在运行使用多代理环境的实验时，我们在“flow_params”字典中指定某些参数。\n",
    "\n",
    "Similar to any other experiments, the following snippets of codes will be inserted into a blank python file (e.g. `new_multiagent_experiment.py`, and should be saved under `flow/examples/exp_configs/rl/multiagent/` directory. (all the basic imports and initialization of variables are omitted in this example for brevity)\n",
    "\n",
    "与其他实验类似，下面的代码片段将被插入到一个空白的python文件中(例如“new_multiagent_experiment。，应该保存在“flow/examples/exp_configs/rl/multiagent/”目录下。(为了简单起见，本例中省略了所有基本的变量导入和初始化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.envs.multiagent import MultiWaveAttenuationPOEnv\n",
    "from flow.networks import MultiRingNetwork\n",
    "from flow.core.params import SumoParams, EnvParams, NetParams, VehicleParams, InitialConfig\n",
    "from flow.controllers import ContinuousRouter, IDMController, RLController\n",
    "\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 3000\n",
    "# Number of rings\n",
    "NUM_RINGS = 1\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "for i in range(NUM_RINGS):\n",
    "    vehicles.add(\n",
    "        veh_id='human_{}'.format(i),\n",
    "        acceleration_controller=(IDMController, {\n",
    "            'noise': 0.2\n",
    "        }),\n",
    "        routing_controller=(ContinuousRouter, {}),\n",
    "        num_vehicles=21)\n",
    "    vehicles.add(\n",
    "        veh_id='rl_{}'.format(i),\n",
    "        acceleration_controller=(RLController, {}),\n",
    "        routing_controller=(ContinuousRouter, {}),\n",
    "        num_vehicles=1)\n",
    "\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag='multiagent_ring_road',\n",
    "\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=MultiWaveAttenuationPOEnv,\n",
    "\n",
    "    # name of the network class the experiment is running on\n",
    "    network=MultiRingNetwork,\n",
    "\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=SumoParams(\n",
    "        sim_step=0.1,\n",
    "        render=False,\n",
    "    ),\n",
    "\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        horizon=HORIZON,\n",
    "        warmup_steps=750,\n",
    "        additional_params={\n",
    "            'max_accel': 1,\n",
    "            'max_decel': 1,\n",
    "            'ring_length': [230, 230],\n",
    "            'target_velocity': 4\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # network-related parameters \n",
    "    net=NetParams(\n",
    "        additional_params={\n",
    "            'length': 230,\n",
    "            'lanes': 1,\n",
    "            'speed_limit': 30,\n",
    "            'resolution': 40,\n",
    "            'num_rings': NUM_RINGS\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # vehicles to be placed in the network at the start of a rollout\n",
    "    veh=vehicles,\n",
    "\n",
    "    # parameters specifying the positioning of vehicles upon initialization/\n",
    "    # reset\n",
    "    initial=InitialConfig(bunching=20.0, spacing='custom'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the following code to create the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.utils.registry import make_create_env\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env\n",
    "register_env(env_name, create_env)\n",
    "\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Shared policies 共享策略\n",
    "\n",
    "When we run a shared-policy multiagent experiment, we refer to the same policy for each agent. In the example below the agents will use 'av' policy.\n",
    "\n",
    "当我们运行共享策略多代理实验时，我们为每个代理引用相同的策略。在下面的示例中，代理将使用“av”策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy\n",
    "\n",
    "def gen_policy():\n",
    "    \"\"\"Generate a policy in RLlib.\"\"\"\n",
    "    return PPOTFPolicy, obs_space, act_space, {}\n",
    "\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "POLICY_GRAPHS = {'av': gen_policy()}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(_):\n",
    "    \"\"\"Map a policy in RLlib.\"\"\"\n",
    "    return 'av'\n",
    "\n",
    "\n",
    "POLICIES_TO_TRAIN = ['av']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Non-shared policies 非共享策略\n",
    "\n",
    "When we run the non-shared multiagent experiment, we refer to different policies for each agent. In the example below, the policy graph will have two policies, 'adversary' and 'av' (non-adversary).\n",
    "\n",
    "当我们运行非共享多代理实验时，我们引用每个代理的不同策略。在下面的示例中，策略图将有两个策略，“对手”和“av”(非对手)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_policy():\n",
    "    \"\"\"Generate a policy in RLlib.\"\"\"\n",
    "    return PPOTFPolicy, obs_space, act_space, {}\n",
    "\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "POLICY_GRAPHS = {'av': gen_policy(), 'adversary': gen_policy()}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    \"\"\"Map a policy in RLlib.\"\"\"\n",
    "    return agent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, just like any other experiments, we run our code using `train_rllib.py` as follows:\n",
    "最后，与其他实验一样，我们使用' train_rllib运行代码。py”如下:\n",
    "\n",
    "    python flow/examples/train_rllib.py new_multiagent_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
